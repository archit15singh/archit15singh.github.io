<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Forging AI’s Lasting Memory: Lessons from Asimov and First Principles | Archit's Space</title><meta name=keywords content="Memory"><meta name=description content="A deep dive into why modern AI agents forget, how Isaac Asimov’s positronic robots managed memory, and a first-principles guide to building memory systems that endure across sessions."><meta name=author content="Archit Singh"><link rel=canonical href=https://archit15singh.github.io/posts/2024-01-01-ai-memory/><link crossorigin=anonymous href=/assets/css/stylesheet.min.29b2809f521134e479bd6ede264ae1617ba99671b2d585b618a26aaededb3374.css integrity="sha256-KbKAn1IRNOR5vW7eJkrhYXuplnGy1YW2GKJqrt7bM3Q=" rel="preload stylesheet" as=style><link rel=icon href=https://archit15singh.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://archit15singh.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://archit15singh.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://archit15singh.github.io/apple-touch-icon.png><link rel=mask-icon href=https://archit15singh.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.147.8"><link rel=alternate hreflang=en href=https://archit15singh.github.io/posts/2024-01-01-ai-memory/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Forging AI’s Lasting Memory: Lessons from Asimov and First Principles"><meta property="og:description" content="A deep dive into why modern AI agents forget, how Isaac Asimov’s positronic robots managed memory, and a first-principles guide to building memory systems that endure across sessions."><meta property="og:type" content="article"><meta property="og:url" content="https://archit15singh.github.io/posts/2024-01-01-ai-memory/"><meta property="og:image" content="https://archit15singh.github.io/images/uploads/ai-memory-architecture.webp"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-01-01T10:00:00+00:00"><meta property="article:modified_time" content="2024-01-01T10:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://archit15singh.github.io/images/uploads/ai-memory-architecture.webp"><meta name=twitter:title content="Forging AI’s Lasting Memory: Lessons from Asimov and First Principles"><meta name=twitter:description content="A deep dive into why modern AI agents forget, how Isaac Asimov’s positronic robots managed memory, and a first-principles guide to building memory systems that endure across sessions."><meta name=twitter:site content="@https://x.com/archit_singh15"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://archit15singh.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Forging AI’s Lasting Memory: Lessons from Asimov and First Principles","item":"https://archit15singh.github.io/posts/2024-01-01-ai-memory/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Forging AI’s Lasting Memory: Lessons from Asimov and First Principles","name":"Forging AI’s Lasting Memory: Lessons from Asimov and First Principles","description":"A deep dive into why modern AI agents forget, how Isaac Asimov’s positronic robots managed memory, and a first-principles guide to building memory systems that endure across sessions.","keywords":["Memory"],"articleBody":"Forging AI’s Lasting Memory: Learning from First Principles and Asimov’s Robots 1. Why AI Agents Today Still “Forget” Despite advances in large language models (LLMs), interacting with most AI chatbots feels like talking to a tide: each new wave of conversation washes away earlier details. Contrast this with Isaac Asimov’s positronic robots, Daneel Olivaw or R. Giskard, who carry knowledge and context across years (or even centuries), never losing sight of a user’s core preferences or a mission’s essential facts.\nWhat’s the root cause of this gap? In modern LLM-based systems, each response hinges on a sliding window of recent tokens. Once the conversation grows too long, everything outside that window disappears. In effect, the AI’s “short-term memory” is a buffer of fixed size, not a true, evolving record. To fix this, we must revisit memory from first principles: How do we decide what to store, when to revise or remove it, and how to fetch just what’s needed?\n2. Memory by Design: Building Blocks from First Principles 2.1. Distilling “What Matters” Human Parallel: When you remember a friend’s favorite ice cream flavor, you don’t archive every detail of their childhood. You focus on the high-impact fact: “They love strawberry.” AI Application: We need mechanisms that parse every user–assistant exchange and surface only the most salient takeaways, for example, “User is allergic to almonds,” or “User assigns highest priority to privacy.” 2.2. Maintaining Coherent Truths Over Time Human Parallel: If your friend once said they were vegetarian but later explained they occasionally eat fish, you update your understanding. You don’t keep both beliefs side by side.\nAI Application: Our system must reconcile conflicting inputs. Instead of accumulating contradictory snippets, introduce a protocol (inspired by Asimov’s careful conflict checks) that can:\nPromote new, higher-priority facts when they override older beliefs. Archive or prune outdated or superseded statements. 2.3. Balancing Depth with Speed Human Parallel: You don’t rummage through every past conversation to recall a friend’s birthday; you consult a concise calendar that highlights birthdays and anniversaries. AI Application: Rather than always re-feeding a multi-thousand–token transcript, craft a “memory index” that points to a handful of key facts. This compressed store speeds up retrieval and reduces per-query cost. 3. Four Fundamental Memory Actions (Reimagined) In Asimov’s stories, a robot’s positronic brain isn’t a passive log; it’s a living ledger that can add, adjust, remove, or bypass information. Translating that into an AI’s memory module, we identify four core operations, but let’s recast them with fresh terminology and nuance:\n☐ “Enshrine” (Add)\nWhat It Means: When a new, impactful fact surfaces (e.g., “User switched from tea to coffee this morning”), the system “enshrines” this detail, storing it in the permanent ledger as a clear entry, dated and labeled. Asimovian Insight: Like a robot marking a noteworthy event in its mission log, the AI chooses to immortalize only the facts that help fulfill its purpose. ☐ “Refine” (Update)\nWhat It Means: If an existing memory needs more detail (e.g., “Previously I said ‘User likes jazz,’ now it’s ‘User loves bebop and blues jazz’”), we “refine” the entry, rewriting or enriching it rather than creating a duplicate. Asimovian Insight: Daneel would refine his understanding of human motivations as new subtleties emerged, never leaving stale or partial insights in his positronic core. ☐ “Fade” (Delete/Deprecate)\nWhat It Means: When a memory conflicts or loses relevance (e.g., “User said they’re vegan, so their earlier ‘vegetarian’ preference must fade”), mark that entry as “faded.” It remains in an archival vault for audit but no longer surfaces in routine retrieval. Asimovian Insight: A robot would “fade” or deactivate outdated protocols that could endanger human welfare, just as our system phases out superseded user attributes. ☐ “Bypass” (No-Op)\nWhat It Means: Sometimes, a freshly extracted nugget doesn’t need any action, if it merely echoes what’s already known (e.g., “User mentions again they love traveling”), the system “bypasses” change, leaving existing knowledge intact. Asimovian Insight: Similar to a robot recognizing that a repeated instruction doesn’t warrant rewriting its mission parameters, some facts simply “ring true” and require no alteration. By reframing Add/Update/Delete/No-Op into Enshrine/Refine/Fade/Bypass, we give each operation a richer identity, one that echoes how a sentient robot might manage its internal worldview.\n4. An Asimov-Inspired Memory Workflow Let’s walk through a scenario borrowing from Asimov’s world, but using our newly minted memory actions:\nSetting: It’s the year 2047, and our AI companion “R3-Angel” assists Dr. Rahman, a physician studying robotic ethics. Over a series of patient consultations and research discussions, R3-Angel must remember key details, past medical decisions, evolving dietary needs, and shifting project priorities.\nInitial Interaction\nDr. Rahman: “My patient, Anika, is lactose intolerant.” Memory Extraction: The system detects a critical health constraint. Action: Enshrine “Anika – allergic_to – Lactose.” Follow-Up\nDr. Rahman: “Actually, she later tested positive for a mild peanut allergy as well.” Memory Extraction: New allergy emerges. Conflict Check: No direct conflict, milk allergy is separate from peanuts. Action: Enshrine “Anika – allergic_to – Peanuts.” Revocation\nDr. Rahman (days later): “Turns out the peanut test was a false positive, she’s okay with peanuts.”\nMemory Extraction: Corrected allergy data.\nConflict Check: “Peanut allergy” contradicts “Peanuts.”\nAction:\nFade the “Peanuts” allergy entry. No need to refine any existing entry, because the new truth is simply the absence of a peanut allergy. Additional Context\nDr. Rahman: “Also, she needs to avoid shellfish during immunotherapy.”\nMemory Extraction: Another dietary restriction.\nAction:\nEnshrine “Anika – allergic_to – Shellfish.” Check for overlap: no conflict, keep the lactate and shellfish constraints independently. Routine Query\nDr. Rahman: “Suggest a dietary plan for Anika during her next chemotherapy session.”\nRetrieval Protocol:\nDense Query: Convert “diet plan Anika chemotherapy” into a vector, fetch nearest enshrined facts, “lactose intolerant,” “shellfish allergy.” Graph Query (Entity-Centric): Identify “Anika” and follow edges to see all “allergic_to” relationships. AI Response: “Recommend a plant-based menu focusing on legumes, vegetables, and dairy substitutes; avoid shellfish and ensure no hidden lactose.”\nAt each stage, R3-Angel’s positronic memory (our AI’s store) systematically enshrines new facts, refines or fades outdated entries, and bypasses redundant noise, mirroring how Asimov’s robots adapt to evolving circumstances.\n5. Beyond CRUD: Enriching Memory with Contextual Layers Asimov’s robots didn’t just hold flat facts, they also understood nuance, emotional tone, and situational importance. To mirror that depth, consider layering our memory store with:\nContext Tags (Why It Matters)\nAttach a “contextual weight” to each enshrined fact. For instance “Anika, shellfish allergy” might carry a “high” tag when discussing chemotherapy, but “medium” when chatting casually about restaurants. This helps retrieval prioritize truly mission-critical memories. Temporal Decay Profiles\nNot all facts deserve equal shelf life. For example, “User is researching a robotics conference in March 2047” becomes obsolete after April 2047. Introduce a “decay timer” that gradually lowers a memory’s retrieval priority unless it’s reaffirmed. Relationship Anchors\nWhen you enshrine “Anika, shellfish allergy,” also link it to “chemotherapy schedule” or “dietary plan.” That way, if a future query asks about drugs or meal recommendations, the system sees the keyword “chemotherapy” and directly retrieves related allergies, skipping over unrelated facets like “Anika’s favorite music.” These enrichments transform a simple ledger into a nuanced, human-like archive, one where facts gain or lose prominence as circumstances evolve, just like a positronic brain’s living tapestry of knowledge.\n6. Crafting a Memory Strategy for Real-World AI How do we translate these ideas into a concrete engineering roadmap? Here’s one possible outline:\nFact Extraction Module\nBuild a lightweight LLM prompt that, given each user–assistant turn, returns up to N candidate facts. Criteria: Look for health constraints, personal preferences, ongoing projects, or commitments, anything that has long-term relevance. Memory Ledger (Dense + Graph Hybrid)\nDense Side: Maintain a collection of text snippets (each fact) with embeddings and metadata (timestamp, context tags). Graph Side: Build a directed graph where nodes are entities (people, places, concepts) and edges denote relationships (e.g., “allergic_to,” “prefers,” “scheduled_for”). Conflict Resolution Engine\nWhen ingesting a new fact, fetch top–k semantically similar entries.\nUse a small classifier (possibly a distilled LLM) to decide:\nEnshrine if novelty is high; Refine if it enriches an existing entry; Fade if it contradicts and nullifies a previous fact; Bypass if it duplicates without change. Pruning \u0026 Decay Policy\nAssign each fact a relevance score based on:\nTime since last access (older → lower). Number of re-affirmations (popular → higher). Context tags that still apply (e.g., “ongoing project” as opposed to “one-time event”). Periodically run a “memory audit” to archive low-scoring facts into a cold-storage bucket (so they can be revived if needed, but not clutter everyday retrieval).\nContextual Retrieval Layer\nFirst-Pass (Dense): For simple lookups, use vector similarity to fetch top–m snippets. Second-Pass (Graph): If the query hints at relationships or multi-hop reasoning (“What diet restrictions should I follow for chemotherapy sessions?”), anchor on entities (“chemotherapy,” “diet restrictions”), traverse edges to gather all relevant nodes (e.g., allergies, metabolic conditions), and hand that subgraph to the LLM. Fusion: Combine snippets and graph-derived facts into a concise context for the LLM to generate a precise, contextual answer. 7. Embracing Asimov’s Long-Term Vision In Asimov’s fiction, robots evolved from isolated machines to companions that understood human nuance, preserved history, and upheld ethical imperatives across generations. Today’s AI agents can begin that transformation by adopting a memory-first architecture, one that enshrines only what matters, refines as truth shifts, fades what’s obsolete, and bypasses redundancy.\nBy embracing these first-principles design decisions, enriched with Asimovian metaphor, we endow AI with:\nConsistency: Never again will an AI innocently recommend lobster chowder to a dairy-allergic patient. Adaptability: When a user’s circumstances change, the AI seamlessly revises its worldview. Efficiency: Conversations scale without ballooning token costs or lag. Trust: Users sense that the agent “remembers them,” forging deeper rapport. Ultimately, building AI that truly remembers isn’t just a engineering challenge, it’s a philosophical one. As Isaac Asimov showed us, memory is the bedrock of identity and purpose. By distilling his visionary robots’ strengths into modern memory protocols, Enshrine, Refine, Fade, Bypass, we move closer to AI companions that aren’t merely reactive, but reliably attuned to our evolving lives, just as Asimov’s positronic characters were to theirs.\n","wordCount":"1684","inLanguage":"en","image":"https://archit15singh.github.io/images/uploads/ai-memory-architecture.webp","datePublished":"2024-01-01T10:00:00Z","dateModified":"2024-01-01T10:00:00Z","author":{"@type":"Person","name":"Archit Singh"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://archit15singh.github.io/posts/2024-01-01-ai-memory/"},"publisher":{"@type":"Organization","name":"Archit's Space","logo":{"@type":"ImageObject","url":"https://archit15singh.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://archit15singh.github.io/ accesskey=h title="Archit's Space (Alt + H)">Archit's Space</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://archit15singh.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://archit15singh.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://archit15singh.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://archit15singh.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://archit15singh.github.io/posts/>Posts</a></div><h1 class=post-title>Forging AI’s Lasting Memory: Lessons from Asimov and First Principles</h1><div class=post-description>A deep dive into why modern AI agents forget, how Isaac Asimov’s positronic robots managed memory, and a first-principles guide to building memory systems that endure across sessions.</div><div class=post-meta><span title="2024-01-01 10:00:00 +0000 UTC">January 1, 2024</span>&nbsp;·&nbsp;8 min</div></header><figure class=entry-cover><img loading=lazy src=https://archit15singh.github.io/images/uploads/ai-memory-architecture.webp alt="Futuristic robot interacting with holographic data nodes"><p>Enshrining, Refining, Fading, and Bypassing: A memory architecture inspired by Asimov’s robots</p></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#forging-ais-lasting-memory-learning-from-first-principles-and-asimovs-robots aria-label="Forging AI’s Lasting Memory: Learning from First Principles and Asimov’s Robots"><strong>Forging AI’s Lasting Memory: Learning from First Principles and Asimov’s Robots</strong></a><ul><ul><li><a href=#1-why-ai-agents-today-still-forget aria-label="1. Why AI Agents Today Still “Forget”">1. Why AI Agents Today Still “Forget”</a></li><li><a href=#2-memory-by-design-building-blocks-from-first-principles aria-label="2. Memory by Design: Building Blocks from First Principles">2. Memory by Design: Building Blocks from First Principles</a><ul><li><a href=#21-distilling-what-matters aria-label="2.1. Distilling “What Matters”">2.1. Distilling “What Matters”</a></li><li><a href=#22-maintaining-coherent-truths-over-time aria-label="2.2. Maintaining Coherent Truths Over Time">2.2. Maintaining Coherent Truths Over Time</a></li><li><a href=#23-balancing-depth-with-speed aria-label="2.3. Balancing Depth with Speed">2.3. Balancing Depth with Speed</a></li></ul></li><li><a href=#3-four-fundamental-memory-actions-reimagined aria-label="3. Four Fundamental Memory Actions (Reimagined)">3. Four Fundamental Memory Actions (Reimagined)</a></li><li><a href=#4-an-asimov-inspired-memory-workflow aria-label="4. An Asimov-Inspired Memory Workflow">4. An Asimov-Inspired Memory Workflow</a></li><li><a href=#5-beyond-crud-enriching-memory-with-contextual-layers aria-label="5. Beyond CRUD: Enriching Memory with Contextual Layers">5. Beyond CRUD: Enriching Memory with Contextual Layers</a></li><li><a href=#6-crafting-a-memory-strategy-for-real-world-ai aria-label="6. Crafting a Memory Strategy for Real-World AI">6. Crafting a Memory Strategy for Real-World AI</a></li><li><a href=#7-embracing-asimovs-long-term-vision aria-label="7. Embracing Asimov’s Long-Term Vision">7. Embracing Asimov’s Long-Term Vision</a></li></ul></ul></li></ul></div></details></div><div class=post-content><h1 id=forging-ais-lasting-memory-learning-from-first-principles-and-asimovs-robots><strong>Forging AI’s Lasting Memory: Learning from First Principles and Asimov’s Robots</strong><a hidden class=anchor aria-hidden=true href=#forging-ais-lasting-memory-learning-from-first-principles-and-asimovs-robots>#</a></h1><hr><h3 id=1-why-ai-agents-today-still-forget>1. Why AI Agents Today Still “Forget”<a hidden class=anchor aria-hidden=true href=#1-why-ai-agents-today-still-forget>#</a></h3><p>Despite advances in large language models (LLMs), interacting with most AI chatbots feels like talking to a tide: each new wave of conversation washes away earlier details. Contrast this with Isaac Asimov’s positronic robots, Daneel Olivaw or R. Giskard, who carry knowledge and context across years (or even centuries), never losing sight of a user’s core preferences or a mission’s essential facts.</p><p>What’s the root cause of this gap? In modern LLM-based systems, each response hinges on a sliding window of recent tokens. Once the conversation grows too long, everything outside that window disappears. In effect, the AI’s “short-term memory” is a buffer of fixed size, not a true, evolving record. To fix this, we must revisit memory from first principles: How do we decide <strong>what to store</strong>, <strong>when to revise or remove it</strong>, and <strong>how to fetch just what’s needed</strong>?</p><hr><h3 id=2-memory-by-design-building-blocks-from-first-principles>2. Memory by Design: Building Blocks from First Principles<a hidden class=anchor aria-hidden=true href=#2-memory-by-design-building-blocks-from-first-principles>#</a></h3><h4 id=21-distilling-what-matters>2.1. Distilling “What Matters”<a hidden class=anchor aria-hidden=true href=#21-distilling-what-matters>#</a></h4><ul><li><strong>Human Parallel:</strong> When you remember a friend’s favorite ice cream flavor, you don’t archive every detail of their childhood. You focus on the high-impact fact: “They love strawberry.”</li><li><strong>AI Application:</strong> We need mechanisms that parse every user–assistant exchange and surface only the most <strong>salient takeaways</strong>, for example, “User is allergic to almonds,” or “User assigns highest priority to privacy.”</li></ul><h4 id=22-maintaining-coherent-truths-over-time>2.2. Maintaining Coherent Truths Over Time<a hidden class=anchor aria-hidden=true href=#22-maintaining-coherent-truths-over-time>#</a></h4><ul><li><p><strong>Human Parallel:</strong> If your friend once said they were vegetarian but later explained they occasionally eat fish, you update your understanding. You don’t keep both beliefs side by side.</p></li><li><p><strong>AI Application:</strong> Our system must reconcile conflicting inputs. Instead of accumulating contradictory snippets, introduce a protocol (inspired by Asimov’s careful conflict checks) that can:</p><ol><li><strong>Promote</strong> new, higher-priority facts when they override older beliefs.</li><li><strong>Archive</strong> or <strong>prune</strong> outdated or superseded statements.</li></ol></li></ul><h4 id=23-balancing-depth-with-speed>2.3. Balancing Depth with Speed<a hidden class=anchor aria-hidden=true href=#23-balancing-depth-with-speed>#</a></h4><ul><li><strong>Human Parallel:</strong> You don’t rummage through every past conversation to recall a friend’s birthday; you consult a concise calendar that highlights birthdays and anniversaries.</li><li><strong>AI Application:</strong> Rather than always re-feeding a multi-thousand–token transcript, craft a “memory index” that points to a handful of key facts. This compressed store speeds up retrieval and reduces per-query cost.</li></ul><hr><h3 id=3-four-fundamental-memory-actions-reimagined>3. Four Fundamental Memory Actions (Reimagined)<a hidden class=anchor aria-hidden=true href=#3-four-fundamental-memory-actions-reimagined>#</a></h3><p>In Asimov’s stories, a robot’s positronic brain isn’t a passive log; it’s a living ledger that can add, adjust, remove, or bypass information. Translating that into an AI’s memory module, we identify four core operations, but let’s recast them with fresh terminology and nuance:</p><ol><li><p><strong>☐ “Enshrine” (Add)</strong></p><ul><li><strong>What It Means:</strong> When a new, impactful fact surfaces (e.g., “User switched from tea to coffee this morning”), the system “enshrines” this detail, storing it in the permanent ledger as a clear entry, dated and labeled.</li><li><strong>Asimovian Insight:</strong> Like a robot marking a noteworthy event in its mission log, the AI chooses to immortalize only the facts that help fulfill its purpose.</li></ul></li><li><p><strong>☐ “Refine” (Update)</strong></p><ul><li><strong>What It Means:</strong> If an existing memory needs more detail (e.g., “Previously I said ‘User likes jazz,’ now it’s ‘User loves bebop and blues jazz’”), we “refine” the entry, rewriting or enriching it rather than creating a duplicate.</li><li><strong>Asimovian Insight:</strong> Daneel would refine his understanding of human motivations as new subtleties emerged, never leaving stale or partial insights in his positronic core.</li></ul></li><li><p><strong>☐ “Fade” (Delete/Deprecate)</strong></p><ul><li><strong>What It Means:</strong> When a memory conflicts or loses relevance (e.g., “User said they’re vegan, so their earlier ‘vegetarian’ preference must fade”), mark that entry as “faded.” It remains in an archival vault for audit but no longer surfaces in routine retrieval.</li><li><strong>Asimovian Insight:</strong> A robot would “fade” or deactivate outdated protocols that could endanger human welfare, just as our system phases out superseded user attributes.</li></ul></li><li><p><strong>☐ “Bypass” (No-Op)</strong></p><ul><li><strong>What It Means:</strong> Sometimes, a freshly extracted nugget doesn’t need any action, if it merely echoes what’s already known (e.g., “User mentions again they love traveling”), the system “bypasses” change, leaving existing knowledge intact.</li><li><strong>Asimovian Insight:</strong> Similar to a robot recognizing that a repeated instruction doesn’t warrant rewriting its mission parameters, some facts simply “ring true” and require no alteration.</li></ul></li></ol><p>By reframing Add/Update/Delete/No-Op into Enshrine/Refine/Fade/Bypass, we give each operation a richer identity, one that echoes how a sentient robot might manage its internal worldview.</p><hr><h3 id=4-an-asimov-inspired-memory-workflow>4. An Asimov-Inspired Memory Workflow<a hidden class=anchor aria-hidden=true href=#4-an-asimov-inspired-memory-workflow>#</a></h3><p>Let’s walk through a scenario borrowing from Asimov’s world, but using our newly minted memory actions:</p><blockquote><p><strong>Setting:</strong> It’s the year 2047, and our AI companion “R3-Angel” assists Dr. Rahman, a physician studying robotic ethics. Over a series of patient consultations and research discussions, R3-Angel must remember key details, past medical decisions, evolving dietary needs, and shifting project priorities.</p></blockquote><ol><li><p><strong>Initial Interaction</strong></p><ul><li><strong>Dr. Rahman:</strong> “My patient, Anika, is lactose intolerant.”</li><li><strong>Memory Extraction:</strong> The system detects a critical health constraint.</li><li><strong>Action:</strong> <strong>Enshrine</strong> “Anika – allergic_to – Lactose.”</li></ul></li><li><p><strong>Follow-Up</strong></p><ul><li><strong>Dr. Rahman:</strong> “Actually, she later tested positive for a mild peanut allergy as well.”</li><li><strong>Memory Extraction:</strong> New allergy emerges.</li><li><strong>Conflict Check:</strong> No direct conflict, milk allergy is separate from peanuts.</li><li><strong>Action:</strong> <strong>Enshrine</strong> “Anika – allergic_to – Peanuts.”</li></ul></li><li><p><strong>Revocation</strong></p><ul><li><p><strong>Dr. Rahman (days later):</strong> “Turns out the peanut test was a false positive, she’s okay with peanuts.”</p></li><li><p><strong>Memory Extraction:</strong> Corrected allergy data.</p></li><li><p><strong>Conflict Check:</strong> “Peanut allergy” contradicts “Peanuts.”</p></li><li><p><strong>Action:</strong></p><ul><li><strong>Fade</strong> the “Peanuts” allergy entry.</li><li>No need to refine any existing entry, because the new truth is simply the absence of a peanut allergy.</li></ul></li></ul></li><li><p><strong>Additional Context</strong></p><ul><li><p><strong>Dr. Rahman:</strong> “Also, she needs to avoid shellfish during immunotherapy.”</p></li><li><p><strong>Memory Extraction:</strong> Another dietary restriction.</p></li><li><p><strong>Action:</strong></p><ul><li><strong>Enshrine</strong> “Anika – allergic_to – Shellfish.”</li><li>Check for overlap: no conflict, keep the lactate and shellfish constraints independently.</li></ul></li></ul></li><li><p><strong>Routine Query</strong></p><ul><li><p><strong>Dr. Rahman:</strong> “Suggest a dietary plan for Anika during her next chemotherapy session.”</p></li><li><p><strong>Retrieval Protocol:</strong></p><ul><li><strong>Dense Query:</strong> Convert “diet plan Anika chemotherapy” into a vector, fetch nearest enshrined facts, “lactose intolerant,” “shellfish allergy.”</li><li><strong>Graph Query (Entity-Centric):</strong> Identify “Anika” and follow edges to see all “allergic_to” relationships.</li></ul></li><li><p><strong>AI Response:</strong> “Recommend a plant-based menu focusing on legumes, vegetables, and dairy substitutes; avoid shellfish and ensure no hidden lactose.”</p></li></ul></li></ol><p>At each stage, R3-Angel’s positronic memory (our AI’s store) systematically enshrines new facts, refines or fades outdated entries, and bypasses redundant noise, mirroring how Asimov’s robots adapt to evolving circumstances.</p><hr><h3 id=5-beyond-crud-enriching-memory-with-contextual-layers>5. Beyond CRUD: Enriching Memory with Contextual Layers<a hidden class=anchor aria-hidden=true href=#5-beyond-crud-enriching-memory-with-contextual-layers>#</a></h3><p>Asimov’s robots didn’t just hold flat facts, they also understood nuance, emotional tone, and situational importance. To mirror that depth, consider layering our memory store with:</p><ol><li><p><strong>Context Tags</strong> (Why It Matters)</p><ul><li>Attach a “contextual weight” to each enshrined fact. For instance “Anika, shellfish allergy” might carry a “high” tag when discussing chemotherapy, but “medium” when chatting casually about restaurants. This helps retrieval prioritize truly mission-critical memories.</li></ul></li><li><p><strong>Temporal Decay Profiles</strong></p><ul><li>Not all facts deserve equal shelf life. For example, “User is researching a robotics conference in March 2047” becomes obsolete after April 2047. Introduce a <strong>“decay timer”</strong> that gradually lowers a memory’s retrieval priority unless it’s reaffirmed.</li></ul></li><li><p><strong>Relationship Anchors</strong></p><ul><li>When you enshrine “Anika, shellfish allergy,” also link it to “chemotherapy schedule” or “dietary plan.” That way, if a future query asks about drugs or meal recommendations, the system sees the keyword “chemotherapy” and directly retrieves related allergies, skipping over unrelated facets like “Anika’s favorite music.”</li></ul></li></ol><p>These enrichments transform a simple ledger into a <strong>nuanced, human-like archive</strong>, one where facts gain or lose prominence as circumstances evolve, just like a positronic brain’s living tapestry of knowledge.</p><hr><h3 id=6-crafting-a-memory-strategy-for-real-world-ai>6. Crafting a Memory Strategy for Real-World AI<a hidden class=anchor aria-hidden=true href=#6-crafting-a-memory-strategy-for-real-world-ai>#</a></h3><p>How do we translate these ideas into a concrete engineering roadmap? Here’s one possible outline:</p><ol><li><p><strong>Fact Extraction Module</strong></p><ul><li>Build a lightweight LLM prompt that, given each user–assistant turn, returns up to <em>N</em> candidate facts.</li><li>Criteria: Look for health constraints, personal preferences, ongoing projects, or commitments, anything that has long-term relevance.</li></ul></li><li><p><strong>Memory Ledger (Dense + Graph Hybrid)</strong></p><ul><li><strong>Dense Side:</strong> Maintain a collection of text snippets (each fact) with embeddings and metadata (timestamp, context tags).</li><li><strong>Graph Side:</strong> Build a directed graph where nodes are entities (people, places, concepts) and edges denote relationships (e.g., “allergic_to,” “prefers,” “scheduled_for”).</li></ul></li><li><p><strong>Conflict Resolution Engine</strong></p><ul><li><p>When ingesting a new fact, fetch top–k semantically similar entries.</p></li><li><p>Use a small classifier (possibly a distilled LLM) to decide:</p><ul><li><strong>Enshrine</strong> if novelty is high;</li><li><strong>Refine</strong> if it enriches an existing entry;</li><li><strong>Fade</strong> if it contradicts and nullifies a previous fact;</li><li><strong>Bypass</strong> if it duplicates without change.</li></ul></li></ul></li><li><p><strong>Pruning & Decay Policy</strong></p><ul><li><p>Assign each fact a <strong>relevance score</strong> based on:</p><ul><li>Time since last access (older → lower).</li><li>Number of re-affirmations (popular → higher).</li><li>Context tags that still apply (e.g., “ongoing project” as opposed to “one-time event”).</li></ul></li><li><p>Periodically run a “memory audit” to <strong>archive</strong> low-scoring facts into a cold-storage bucket (so they can be revived if needed, but not clutter everyday retrieval).</p></li></ul></li><li><p><strong>Contextual Retrieval Layer</strong></p><ul><li><strong>First-Pass (Dense):</strong> For simple lookups, use vector similarity to fetch top–m snippets.</li><li><strong>Second-Pass (Graph):</strong> If the query hints at relationships or multi-hop reasoning (“What diet restrictions should I follow for chemotherapy sessions?”), anchor on entities (“chemotherapy,” “diet restrictions”), traverse edges to gather all relevant nodes (e.g., allergies, metabolic conditions), and hand that subgraph to the LLM.</li><li><strong>Fusion:</strong> Combine snippets and graph-derived facts into a concise context for the LLM to generate a precise, contextual answer.</li></ul></li></ol><hr><h3 id=7-embracing-asimovs-long-term-vision>7. Embracing Asimov’s Long-Term Vision<a hidden class=anchor aria-hidden=true href=#7-embracing-asimovs-long-term-vision>#</a></h3><p>In Asimov’s fiction, robots evolved from isolated machines to companions that understood human nuance, preserved history, and upheld ethical imperatives across generations. Today’s AI agents can begin that transformation by adopting a <strong>memory-first architecture</strong>, one that enshrines only what matters, refines as truth shifts, fades what’s obsolete, and bypasses redundancy.</p><p>By embracing these first-principles design decisions, enriched with Asimovian metaphor, we endow AI with:</p><ul><li><strong>Consistency:</strong> Never again will an AI innocently recommend lobster chowder to a dairy-allergic patient.</li><li><strong>Adaptability:</strong> When a user’s circumstances change, the AI seamlessly revises its worldview.</li><li><strong>Efficiency:</strong> Conversations scale without ballooning token costs or lag.</li><li><strong>Trust:</strong> Users sense that the agent “remembers them,” forging deeper rapport.</li></ul><p>Ultimately, building AI that truly remembers isn’t just a engineering challenge, it’s a philosophical one. As Isaac Asimov showed us, memory is the bedrock of identity and purpose. By distilling his visionary robots’ strengths into modern memory protocols, Enshrine, Refine, Fade, Bypass, we move closer to AI companions that aren’t merely reactive, but reliably attuned to our evolving lives, just as Asimov’s positronic characters were to theirs.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://archit15singh.github.io/tags/memory/>Memory</a></li></ul><nav class=paginav><a class=next href=https://archit15singh.github.io/posts/2023-01-01-my-reading-list/><span class=title>Next Page »</span><br><span>My Reading List for the next 10 years and Why</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Forging AI’s Lasting Memory: Lessons from Asimov and First Principles on twitter" href="https://twitter.com/intent/tweet/?text=Forging%20AI%e2%80%99s%20Lasting%20Memory%3a%20Lessons%20from%20Asimov%20and%20First%20Principles&amp;url=https%3a%2f%2farchit15singh.github.io%2fposts%2f2024-01-01-ai-memory%2f&amp;hashtags=Memory"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Forging AI’s Lasting Memory: Lessons from Asimov and First Principles on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2farchit15singh.github.io%2fposts%2f2024-01-01-ai-memory%2f&amp;title=Forging%20AI%e2%80%99s%20Lasting%20Memory%3a%20Lessons%20from%20Asimov%20and%20First%20Principles&amp;summary=Forging%20AI%e2%80%99s%20Lasting%20Memory%3a%20Lessons%20from%20Asimov%20and%20First%20Principles&amp;source=https%3a%2f%2farchit15singh.github.io%2fposts%2f2024-01-01-ai-memory%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Forging AI’s Lasting Memory: Lessons from Asimov and First Principles on reddit" href="https://reddit.com/submit?url=https%3a%2f%2farchit15singh.github.io%2fposts%2f2024-01-01-ai-memory%2f&title=Forging%20AI%e2%80%99s%20Lasting%20Memory%3a%20Lessons%20from%20Asimov%20and%20First%20Principles"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Forging AI’s Lasting Memory: Lessons from Asimov and First Principles on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2farchit15singh.github.io%2fposts%2f2024-01-01-ai-memory%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Forging AI’s Lasting Memory: Lessons from Asimov and First Principles on whatsapp" href="https://api.whatsapp.com/send?text=Forging%20AI%e2%80%99s%20Lasting%20Memory%3a%20Lessons%20from%20Asimov%20and%20First%20Principles%20-%20https%3a%2f%2farchit15singh.github.io%2fposts%2f2024-01-01-ai-memory%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Forging AI’s Lasting Memory: Lessons from Asimov and First Principles on telegram" href="https://telegram.me/share/url?text=Forging%20AI%e2%80%99s%20Lasting%20Memory%3a%20Lessons%20from%20Asimov%20and%20First%20Principles&amp;url=https%3a%2f%2farchit15singh.github.io%2fposts%2f2024-01-01-ai-memory%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><div class=container><div class="content has-text-centered"><p><strong>Archit's Space</strong> - Sharing knowledge one post at a time.<br>Crafted with ❤️ by Archit Singh</a>.<br>Connect with me on
<a href=https://github.com/archit15singh target=_blank>GitHub</a>,
<a href=https://www.linkedin.com/in/archit15singh target=_blank>LinkedIn</a>,
and <a href=https://twitter.com/archit15singh target=_blank>Twitter</a>.</p></div></div></footer><script data-goatcounter=https://architsingh.goatcounter.com/count async src=//gc.zgo.at/count.js></script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function s(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>